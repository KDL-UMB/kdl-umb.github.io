---
layout: page
title: 2021-08-19
---
- It is particularly helpful when project reports furnish a broad description of the research progress in terms that are understandable to a general scientist (as opposed to a disciplinary specialist).
- Human evaluations are typically considered the gold standard in natural language generation, but as models’ fluency improves, how well can evaluators detect and judge machine-generated text? We run a study assessing nonexperts’ ability to distinguish between human and machine-authored text (GPT2 and GPT3) in three domains (stories, news articles, and recipes).
- The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.[R. Sutton, “The Bitter Lesson,”](http://incompleteideas.net/IncIdeas/BitterLesson.html) 
- I believe that extending the ablation experiment to disentangle the effects of the novel aggregation scheme from those of the attention mechanism would be instrumental in order to provide sufficient insight on why and how the proposed approach outperforms the state of the art.
- While statistical methods are mathematically well acceptable, fast to evaluate, and easier to implement, their results could be unreliable for practical applications due to their dependency and assumption of a specific distribution model.
