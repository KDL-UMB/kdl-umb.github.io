---
layout: page
title: 2021-10-15
---
#### **Example Sentences**:
- **Hefei Qiu:** One difficulty that arises with such an experimental setup is being able to construct a large enough word vocabulary to encode arbitrary sentences. For example, a sentence from a Wikipedia article might contain nouns that are highly unlikely to appear in our book vocabulary. We solve this problem by learning a mapping that transfers word representations from one model to another.[source](https://arxiv.org/pdf/1506.06726.pdf)
- **Yong Zhuang:** Humans and other animals have a **remarkable** ability to take in data from many sources, **integrate** it seamlessly, and deploy it flexibly in the service of a range of goals. **Yet most** machine learning research focuses on building bespoke systems to handle the stereotyped set of inputs and outputs associated with a single task.[source](https://arxiv.org/pdf/2107.14795.pdf)
- **Wei Ding:** This interplay between substance and structure can be demonstrated by something as basic as a simple table. Let us say that in tracking the temperature of a liquid over a period of time, an investigator takes measurements every three minutes and records a list of temperatures. Those data could be presented by a number of written structures.[source](https://www.americanscientist.org/blog/the-long-view/the-science-of-scientific-writing)


#### **Before & After**:
- Tianyu Kang
  - **before**: We define the generalizability of neural network models as the activation ratio of each inner single unit or feature map. Our experiments benchmark datasets across both classical neural network architectures as well as SOTA algorithms. The experiment results show the quality of skills actually improves the quality of generalization.

  - **after**: We argue here that the generalizability of neural network models can be deconstructed by the activation ratio of each inner single unit or feature map; Besides the theoretical guarantees, we demonstrate a number of empirical validations on the proposed algorithm on benchmark datasets across both classical neural network architectures as well as SOTA algorithms. The results are meaningful, not merely inessential: Improving the quality of skills actually improves the quality of generalization.

- Zihan Li
  - **before**: (Identifying the internal relationships in the data is the basis of data analysis and prediction.) Traditional statistics methods focus on testing the correlation of variables pairwise. However, the correlation has rather limited performance on real causal influence.

  - **after**: We argue that the traditional statistics methods having limited contribution to exploring the internal causal relationships between variables, and in fact may mislead researchers as they focus on testing the correlation of variables pairwise rather than performing as well as possible on cause and effect.

- Yong Zhuang
  - **before**: Although existing machine learning methods have been successful in many fields, such as climate change, finance, and disease control, the underline physical laws of these phenomena are still complex and hard to learn.

  - **after**: Although machine learning has become more critical to efforts in science in engineering, such as understanding the neural basis of cognition, extracting and predicting coherent changes in the climate, stabilizing financial markets, managing the spread of disease, and controlling turbulence, where data are abundant, physical laws remain elusive.

- Chengjie Zheng
  - **before**: In this paper,  we propose an approach for behavior recognition modeling and detection of certain types of anomalous behavior.

  - **after**: We present a conceptually simple, lightweight, and general framework for behavior recognition in video
