---
layout: page
title: 2021-10-29
---
#### **Before & After**:
- Hefei Qiu
  - **before**: Different from previous methods of using data augmentation to construct positive pairs in contrastive learning, we use semantic similarity or entailment relation of two sentences to build positive pairs.

  - **after**: In the way of constructing positive pairs, different from previous methods of using data augmentation which are usually sophisticated or uninterpretable, we apply semantic similarity or entailment relation of two sentences which is easy to implement and easy to understand.

- Tianyu Kang
  - **before**: This idea comes from the natural structure of the neural network. Since we borrow the structure from humans' real neural networks in the brain, we think we can borrow more prosperity from the human learning process.

  - **after**: It’s an analogy that goes back to the dawn of the artificial neural network: ever since we discovered that humans could solve problems by neural networks in the brain, we’ve wondered if the machine might work in a similar fashion.

- Zihan Li
  - **before**: To diagnose and consider relationships between numerous features sequentially, we use conditional independent test, which follows the Bayesian rule, a powerful non-parametric method detecting and revealing causal relationships between variables, but considers the effects from previous variables instead of traditional independent test.

  - **after**: To diagnose and consider relationships between numerous features sequentially, we develop a framework based on the conditional Bayesian independent test with two advantaged factors: detecting and revealing causal relationships between variables, considering the effects from previous variables instead of traditional independent tests.





