---
layout: page
title: 2022-02-04
---

#### **Example Sentences**:

- **Yong Zhuang:** Introducing and integrating more comprehensive information is crucial to making more accurate predictions.[source](https://www.ijcai.org/proceedings/2021/0518.pdf)

- **Hefei Qiu:** In this section, we walk through
the key arguments for how benchmarking is a limited approach to assess general model capabilities
and, in particular, discuss the risk of making this claim to generality in the context of the *limited task
design*, *de-contextualized data and performance reporting* as well as *inappropriate community use*
common to such benchmarks in the ML context. [source](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/084b6fbb10729ed4da8c3d3f5a3ae7c9-Paper-round2.pdf)

- **Zihan Li:** For fine-tuning on input with larger resolution, we employ an adamW optimizer for 30 epochs with a constant learning rate of $10^{−5}$, weight decay of $10^{−8}$, and the same data augmentation and regularizations as the first stage except for setting the stochastic depth ratio to 0.1.[source](https://arxiv.org/pdf/2103.14030.pdf)




#### **Before & After**:
- Hefei Qiu
  - Through an extensive empirical study on four major NLP downstream tasks including machine translation, summarization, text simplification, and natural language inference, we find that our proposed evaluation metric correlates with human judgments while not requiring any reference labels.

- Zihan Li
  - **before**: The baseline DNN classifier is built with the same 3-layer fully connected neural network as used in our method.
  - **after**: For the purpose of comparison with the contrast experiment, we built a baseline DNN classifier with 3-layer fully connected network, kept the same weights as used in our method, and the same data engineering processing as the stage in our method except for the summarization.






